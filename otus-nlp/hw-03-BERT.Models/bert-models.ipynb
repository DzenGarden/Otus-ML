{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28181fe4-2250-4ba4-8a4e-23de3fcf9e15",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "Почувствуй мощь трансформеров в бою\n",
    "\n",
    "**Цель**:\n",
    "\n",
    "Научиться работать с трансформерными моделями и применять их для различных NLP задач.\n",
    "\n",
    "**Описание/Пошаговая инструкция выполнения домашнего задания:**\n",
    "\n",
    "В качестве данных выберете возьмите датасет RuCoLA для русского языка https://github.com/RussianNLP/RuCoLA (в качестве train возьмите in_domain_train.csv, а в качестве теста in_domain_dev.csv).\n",
    "\n",
    "Разбейте in_domain_train на train и val.\n",
    "\n",
    "1. Зафайнтьюньте и протестируйте RuBert или RuRoBerta на данной задаче (можно взять любую предобученную модель руберт с сайта huggingface. Например, ruBert-base/large https://huggingface.co/sberbank-ai/ruBert-base / https://huggingface.co/sberbank-ai/ruBert-large или rubert-base-cased https://huggingface.co/DeepPavlov/rubert-base-cased, ruRoberta-large https://huggingface.co/sberbank-ai/ruRoberta-large, xlm-roberta-base https://huggingface.co/xlm-roberta-base).\n",
    "\n",
    "2. Возьмите RuGPT3 base или large и решите данное задание с помощью методов few-/zero-shot.\n",
    "\n",
    "а) переберите несколько вариантов затравок;\n",
    "\n",
    "б) протестируйте различное число few-shot примеров (0, 1, 2, 4).\n",
    "\n",
    "3. Обучите и протестируйте модель RuT5 на данной задаче (пример finetun’а можете найти здесь https://github.com/RussianNLP/RuCoLA/blob/main/baselines/finetune_t5.py).\n",
    "\n",
    "Сравните полученные результаты.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0054ba7f-dbe0-400e-a68c-775f73d77618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "import accelerate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from transformers import pipeline, DataCollatorWithPadding\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from razdel import tokenize\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ead79-1eba-48e0-85ee-5ab17a64d8d5",
   "metadata": {},
   "source": [
    "## 1. RuBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab6ac3-2053-413d-ac7c-e2bd9f54cf99",
   "metadata": {},
   "source": [
    "### 1.1 Загружаем датасет RuCoLa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1820f993-21c6-4065-9783-8053dc989257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/in_domain_train.csv', index_col=0)\n",
    "df_test = pd.read_csv('data/in_domain_dev.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfa2de7-8376-4093-ae36-bc554c871684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>error_type</th>\n",
       "      <th>detailed_source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Только Иван выразил какую бы то ни было готовн...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Теперь ты видишь собственными глазами, как тут...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>На поверку вся теория оказалась полной чепухой.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>Установки не было введено в действие.</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>Конечно, против такой системы ценностей решите...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>Симптомов болезни не исчезло.</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7867</th>\n",
       "      <td>Послезавтра температура у больного снижается д...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>Говоря, например, о картине Александра Иванова...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7869 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  acceptable  \\\n",
       "id                                                                    \n",
       "0     Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "1                          Этим летом не никуда ездили.           0   \n",
       "2     Только Иван выразил какую бы то ни было готовн...           1   \n",
       "3     Теперь ты видишь собственными глазами, как тут...           1   \n",
       "4       На поверку вся теория оказалась полной чепухой.           1   \n",
       "...                                                 ...         ...   \n",
       "7864              Установки не было введено в действие.           0   \n",
       "7865  Конечно, против такой системы ценностей решите...           0   \n",
       "7866                      Симптомов болезни не исчезло.           0   \n",
       "7867  Послезавтра температура у больного снижается д...           0   \n",
       "7868  Говоря, например, о картине Александра Иванова...           0   \n",
       "\n",
       "     error_type detailed_source  \n",
       "id                               \n",
       "0             0   Paducheva2004  \n",
       "1        Syntax         Rusgram  \n",
       "2             0   Paducheva2013  \n",
       "3             0   Paducheva2010  \n",
       "4             0   Paducheva2010  \n",
       "...         ...             ...  \n",
       "7864  Semantics   Paducheva2004  \n",
       "7865  Semantics   Paducheva2013  \n",
       "7866  Semantics   Paducheva2013  \n",
       "7867  Semantics         Rusgram  \n",
       "7868  Semantics   Paducheva2013  \n",
       "\n",
       "[7869 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1df174-3e57-43cc-90ca-96fa5f14eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7869 entries, 0 to 7868\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   sentence         7869 non-null   object\n",
      " 1   acceptable       7869 non-null   int64 \n",
      " 2   error_type       7869 non-null   object\n",
      " 3   detailed_source  7869 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 307.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd831432-424a-419f-b3f0-ea68d06a8662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 983 entries, 0 to 982\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   sentence         983 non-null    object\n",
      " 1   acceptable       983 non-null    int64 \n",
      " 2   error_type       983 non-null    object\n",
      " 3   detailed_source  983 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 38.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92e53f5-55c0-4998-bba7-594ba98f0c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"acceptable\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e97a0219-c2f2-40d6-8b26-46b4631d653a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"acceptable\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c0bc1c-134a-4fe2-903c-cde7213744c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>error_type</th>\n",
       "      <th>detailed_source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceptable</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5864</td>\n",
       "      <td>5864</td>\n",
       "      <td>5864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentence  error_type  detailed_source\n",
       "acceptable                                       \n",
       "0               2005        2005             2005\n",
       "1               5864        5864             5864"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby([\"acceptable\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d778386-6100-4e40-884b-384ca945479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acceptable\n",
       "1    5864\n",
       "0    2005\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"acceptable\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96e590ca-490b-4bb8-a207-c70f7e0bf57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"error_type\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf493754-72a3-436f-8f56-0bdf719c4640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', 'Syntax', 'Semantics', 'Morphology'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"error_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b76142e6-1c6a-4e60-bb3d-fc9d63428b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7575"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = random.randint(0, df_train.shape[0]-1)\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8dc6510-999b-40d6-a519-653b7a335e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Добрый поступок создает и накапливает добро, сделает жизнь лучше, развивает гуманность.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sentence[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360195e-6e4d-4432-86d8-8ad326b16898",
   "metadata": {},
   "source": [
    "## 1.1. Подготовим датасет для работы с моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c28d5cf-9c16-4b43-b521-9a63ac424606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 7869\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Dataset.from_dict({'text':df_train.sentence, 'label':df_train.acceptable}, split='train')\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7f8620d-16f8-4333-8064-8fc15964041d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Вдруг решетка беззвучно поехала в сторону, и на балконе возникла таинственная фигура, прячущаяся от лунного света, и погрозила Ивану пальцем.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e8ae8f6-7f51-4cf3-b65f-847f8ac52159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 983\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = Dataset.from_dict({'text':df_test.sentence, 'label':df_test.acceptable}, split='test')\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34ad92da-f67b-40a7-aac7-4d047ab5c4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'На Марсе есть какие-либо (какие бы то ни было) разумные обитатели.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds['text'][982]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966c45e-be34-4a70-9df0-a8462ad7ccd8",
   "metadata": {},
   "source": [
    "## 1.2 Загрузим модель RuBERT с HaggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daf2ed55-743f-467c-bb57-b7b4d8a91f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'ai-forever/ruBert-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44c87a94-4b6a-407a-acf8-47829893e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57af2039-44c1-49ff-b797-2ae5b0fa26b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fb41070-cc0b-4cfd-a30c-3ae728a4403d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d67c408929a46e7994502da900daadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds_tokenized = train_ds.map(lambda x: tokenizer(x['text'], truncation=True, max_length=512), batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac2e4d1f-205b-4557-9adb-77a14b819ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b41726e0d54943b5e23c017ea3577e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/983 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ds_tokenized = test_ds.map(lambda x: tokenizer(x['text'], truncation=True, max_length=512), batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e930f2de-b37a-4674-a7b3-4c31add60a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'input_ids': [101, 104691, 379, 5171, 672, 14207, 126, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41f08029-6157-4594-a632-a486673633e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62f76b17-2924-42b4-bf14-d099c6eddcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds_tokenized, shuffle=True, batch_size=4, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "559b5b23-19b8-420c-85b1-35b83fa0c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_ds_tokenized, shuffle=False, batch_size=4, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59caa58e-3294-40ee-ada0-f1004cce7b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3da68c7-6d6c-4672-a164-c4766f14f807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForSequenceClassification"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0ef3685-0e60-48b8-bba1-7a6906802e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the code device-agnostic\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47ca4367-b1e0-4cf1-a95b-f79c6fe9ef2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a893d69-8874-495e-9619-4e476eb2276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-6)  # with tiny batches, LR should be very small as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d96c3a7a-fc11-40f4-a384-38e33f3f5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b4df7c8-48b0-4977-8402-de28125ef8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e0364840b44651b363c888639a8e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1493fca6041f4221b2b253f29bda4672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cf39a5989347238924f1faae66966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.5661448127031327 Eval Loss: 0.5320105203162364 Accuracy 0.7477110885045778\n"
     ]
    }
   ],
   "source": [
    "# set initial best loss to infinite\n",
    "best_eval_loss = float('inf')\n",
    "\n",
    "# empty list to store loss for each epoch\n",
    "losses = []\n",
    "\n",
    "for epoch in trange(1):  ## !!! Debugging - Do not forget revert back to 5!\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    model.train()\n",
    "    for i, batch in enumerate(pbar):\n",
    "        out = model(**batch.to(model.device))\n",
    "        out.loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(out.loss.item())\n",
    "        pbar.set_description(f'loss: {np.mean(losses[-100:]):2.2f}')\n",
    "\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    eval_preds = []\n",
    "    eval_targets = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "                out = model(**batch.to(model.device))\n",
    "        eval_losses.append(out.loss.item())\n",
    "        eval_preds.extend(out.logits.argmax(1).tolist())\n",
    "        eval_targets.extend(batch['labels'].tolist())\n",
    "    print('Epoch:', epoch+1, 'Train Loss:', np.mean(losses[-100:]), 'Eval Loss:', np.mean(eval_losses), 'Accuracy', np.mean(np.array(eval_targets) == eval_preds))\n",
    "    #save the best model\n",
    "    if np.mean(eval_losses) < best_eval_loss:\n",
    "        best_eval_loss = np.mean(eval_losses)\n",
    "        torch.save(model.state_dict(), 'bert_saved_weights.pt')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8063db-fa45-456c-a3b0-304f0a72d1ed",
   "metadata": {},
   "source": [
    "Видим, что в определенный момент validation loss начинает увеличиваться и это означает, что модель переобучается на нашем небольшом наборе данных. Загрузим сохраненную наилучшую модель и посчитаем метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5a63889-948c-4439-8ff8-63c61b31e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality(true_y, prediction_y, ndig=3):\n",
    "    \"\"\"\n",
    "    Evaluates and returns the following metrics: Accuracy, Precision, Recall, F1-score, AUC\n",
    "    \"\"\"\n",
    "    accuracy = round(accuracy_score(true_y, prediction_y), ndig)\n",
    "    precision = round(precision_score(true_y, prediction_y), ndig)\n",
    "    recall = round(recall_score(true_y, prediction_y), ndig)\n",
    "    f1 = round(f1_score(true_y, prediction_y), ndig)\n",
    "    auc = round(roc_auc_score(true_y, prediction_y), ndig)\n",
    "    print(f\" Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"   Recall: {recall}\")\n",
    "    print(f\" F1-score: {f1}\")\n",
    "    print(f\"      AUC: {auc}\")\n",
    "    return [accuracy, precision, recall, f1, auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc3419a8-25c8-44c5-b045-25ad5fb0a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f2a8c82-2462-4c68-9331-27c46b09f4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'bert_saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13843966-01c9-49f8-abe2-dab1eeead985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9497ba745884394843cf7674b026336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recent train loss 0.5661448127031327 eval loss 0.5320105203162364 accuracy 0.7477110885045778\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_losses = []\n",
    "eval_preds = []\n",
    "eval_targets = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "            out = model(**batch.to(model.device))\n",
    "    eval_losses.append(out.loss.item())\n",
    "    eval_preds.extend(out.logits.argmax(1).tolist())\n",
    "    eval_targets.extend(batch['labels'].tolist())\n",
    "print('recent train loss', np.mean(losses[-100:]), 'eval loss', np.mean(eval_losses), 'accuracy', np.mean(np.array(eval_targets) == eval_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "627236f9-7537-4bff-a5ea-272fcefbd8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       250\n",
      "           1       0.75      1.00      0.86       733\n",
      "\n",
      "    accuracy                           0.75       983\n",
      "   macro avg       0.87      0.50      0.44       983\n",
      "weighted avg       0.81      0.75      0.64       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(eval_targets, eval_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "219f8f98-d955-40f8-b939-f1e7e1983cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.748\n",
      "Precision: 0.747\n",
      "   Recall: 1.0\n",
      " F1-score: 0.855\n",
      "      AUC: 0.504\n"
     ]
    }
   ],
   "source": [
    "results['ruBERT'] = quality(eval_targets, eval_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d37bfccc-7767-4dd0-aa98-db1a4c9b36bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ruBERT</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision  Recall  F1-score    AUC\n",
       "ruBERT     0.748      0.747     1.0     0.855  0.504"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebc14dea-1984-44cf-8bee-47933048c103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391e761-e15e-4fe7-966c-14e381627ce8",
   "metadata": {},
   "source": [
    "### Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc67dc-608f-4fd5-b601-2e95c4423dfe",
   "metadata": {},
   "source": [
    "Для zero-shot классификации воспользуемся стандарным pipeline от Haggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed0fb1-10d1-4d22-b26e-8028e5d864d2",
   "metadata": {},
   "source": [
    "Links (delete later)\n",
    "- [GFG: Zero shot text classification](https://www.geeksforgeeks.org/zero-shot-text-classification-using-huggingface-model/)\n",
    "- [Medium: Map class labels from srings to numbers](https://medium.com/@duzhewang/change-the-class-labels-from-a-string-representation-into-an-integer-format-in-python-using-map-62414d4a1a7e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bf025ee-dcb9-492e-8632-ba80ed64e0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ai-forever/rugpt3large_based_on_gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"ai-forever/rugpt3large_based_on_gpt2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d68ae701-4493-49b0-9135-bca1781eb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Установки не было введено в действие.\"\n",
    "candidate_labels = [\"корректное предложение\", \"некорректное предложение\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be1c6c90-99a0-4790-9446-81dbad908f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Установки не было введено в действие.', 'labels': ['корректное предложение', 'некорректное предложение'], 'scores': [0.5091734528541565, 0.4908265471458435]}\n"
     ]
    }
   ],
   "source": [
    "result = classifier(text, candidate_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6def57fc-0ac9-45fc-8d99-84be9d72ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Иван вчера не позвонил.\"\n",
    "candidate_labels = [\"некорректное предложение\", \"корректное предложение\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1caa88f1-dea0-4e02-863f-d6c96b531266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Иван вчера не позвонил.', 'labels': ['корректное предложение', 'некорректное предложение'], 'scores': [0.5179219245910645, 0.48207807540893555]}\n"
     ]
    }
   ],
   "source": [
    "result = classifier(text, candidate_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59f419bc-5875-4e68-91db-29f3e6921305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_targets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b95d7e0-37e2-4af1-a52d-6eb4f6012e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2873e1a1-643c-40e6-b507-9fb0aab6eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting results in batch режиме\n",
    "zero_shot_out = classifier(test_ds['text'], candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f03e338f-6852-47d1-a591-d04b3b0d3663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(zero_shot_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57cf4422-a424-4648-b0ad-3de5962e6813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['корректное предложение', 'некорректное предложение']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_out[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d353b2bd-4a11-48b6-9a2c-6eaea6eb00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels usin list comprehension\n",
    "first_labels = [item['labels'][0] for item in zero_shot_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8b03058-5f7b-4145-9eae-1db38acd2787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70b59d0c-aa02-4252-9578-1163a3077dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_preds = list(map(lambda x: 1 if x == 'корректное предложение' else 0, first_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19d71f75-32f7-4128-8ae9-b49b98a4c081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e55b0d28-ec9e-40e6-ac4c-c6142d5f8b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.07      0.11       250\n",
      "           1       0.75      0.96      0.84       733\n",
      "\n",
      "    accuracy                           0.73       983\n",
      "   macro avg       0.56      0.51      0.48       983\n",
      "weighted avg       0.65      0.73      0.66       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(eval_targets, zs_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "907a3a53-2fc2-47fd-b8a4-0a4eff949d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.732\n",
      "Precision: 0.751\n",
      "   Recall: 0.959\n",
      " F1-score: 0.842\n",
      "      AUC: 0.514\n"
     ]
    }
   ],
   "source": [
    "results['Zero-shot'] = quality(eval_targets, zs_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62eef544-4b1a-4bb2-a51c-2df9f8c201d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ruBERT</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.747</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zero-shot</th>\n",
       "      <td>0.732</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Accuracy  Precision  Recall  F1-score    AUC\n",
       "ruBERT        0.748      0.747   1.000     0.855  0.504\n",
       "Zero-shot     0.732      0.751   0.959     0.842  0.514"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3cb08d55-bee2-49d1-9bee-2c38f92f32ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ai-forever/rugpt3large_based_on_gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"ai-forever/rugpt3large_based_on_gpt2\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eaa87e-61f3-46b9-a071-e1e2d8a648ee",
   "metadata": {},
   "source": [
    "В zero-shot варианте метрики хуже, попробуем few-shots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3545e5-7c3a-4db8-a123-7200cfa43685",
   "metadata": {},
   "source": [
    "### Few-shots classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfae90-41af-4d00-91e9-6f4d91b92a1c",
   "metadata": {},
   "source": [
    "Используем другой подход - будем вызывать инференс модели с few-shot промптом и считать loss для оценки грамматической корректности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3dec1bbe-a027-493c-b5d5-bcf8fa3c2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/rugpt3large_based_on_gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ai-forever/rugpt3large_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ee0af66-5b3d-430f-8fb7-3efac71e5e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1536)\n",
       "    (wpe): Embedding(2048, 1536)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=4608, nx=1536)\n",
       "          (c_proj): Conv1D(nf=1536, nx=1536)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=6144, nx=1536)\n",
       "          (c_proj): Conv1D(nf=1536, nx=6144)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a361e71-a001-42ae-a4f8-42d134f3c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Иван вчера не позвонил.'\n",
    "few_shots = ['Предложение далее корректное? ' + 'Солнце садилось за горизонт.' + \" Ответ: да.\",\n",
    "             'Предложение далее корректное? ' + 'Не стоит сидеть сложить руки.' + \" Ответ: нет.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b29621a7-409e-4e6c-8b14-2113fa844fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to calculate loss and get predictions\n",
    "def calc_loss(phrase: str,\n",
    "                        tokenizer,\n",
    "                        model):\n",
    "\n",
    "    phrase = tokenizer.encode(phrase)\n",
    "    # Adding <EOS> token in case the given phrase is only 1 token length, to avoid an error\n",
    "    if len(phrase) == 1:\n",
    "         phrase.append(tokenizer.eos_token_id)\n",
    "    phrase = torch.tensor(phrase, dtype=torch.long, device=device)\n",
    "    phrase = phrase.unsqueeze(0)  # .repeat(num_samples, 1)\n",
    "    with torch.no_grad():\n",
    "        loss = model(phrase, labels=phrase)\n",
    "    return loss[0].item()\n",
    "\n",
    "def get_loss_num(text):\n",
    "    loss = calc_loss(phrase=text, model=model, tokenizer=tokenizer)\n",
    "    return loss\n",
    "\n",
    "def get_correct_prompt(phrase, few_shots=few_shots):\n",
    "    return '\\n'.join(few_shots) +'\\nПредложение далее корректное? ' + phrase + \" Ответ: да.\"\n",
    "\n",
    "def get_incorrect_prompt(phrase, few_shots=few_shots):\n",
    "    return '\\n'.join(few_shots) + '\\nПредложение далее корректное? ' + phrase + \" Ответ: нет.\"\n",
    "\n",
    "def get_few_shot_pred(text):\n",
    "    res = {}\n",
    "    #print(get_correct_prompt(text)) ## Debugging\n",
    "    correct_loss = calc_loss(phrase=get_correct_prompt(text), model=model, tokenizer=tokenizer)\n",
    "    #print(f\"Correct Loss: {correct_loss}\") ## Debugiing\n",
    "    \n",
    "    #print(get_incorrect_prompt(text)) ## Debugging\n",
    "    incorrect_loss = calc_loss(phrase=get_incorrect_prompt(text), model=model, tokenizer=tokenizer)\n",
    "    #print(f\"Incorrect Loss: {incorrect_loss}\") ## Debugging\n",
    "\n",
    "    pred_num = 1 if correct_loss < incorrect_loss else 0\n",
    "    \n",
    "    res[\"Correct_Loss\"] = correct_loss\n",
    "    res[\"Inorrect_Loss\"] = incorrect_loss\n",
    "    res[\"pred\"] = pred_num\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f675adea-f395-476c-9a2b-0c778a1f1675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложение далее корректное? Солнце садилось за горизонт. Ответ: да.\n",
      "Предложение далее корректное? Не стоит сидеть сложить руки. Ответ: нет.\n",
      "Предложение далее корректное? Иван вчера не позвонил. Ответ: да.\n"
     ]
    }
   ],
   "source": [
    "correct_prompt = get_correct_prompt(text, few_shots)\n",
    "print(correct_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77f1231a-2da2-4f45-84e5-27fda695f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложение далее корректное? Солнце садилось за горизонт. Ответ: да.\n",
      "Предложение далее корректное? Не стоит сидеть сложить руки. Ответ: нет.\n",
      "Предложение далее корректное? Иван вчера не позвонил. Ответ: нет.\n"
     ]
    }
   ],
   "source": [
    "incorrect_prompt = get_incorrect_prompt(text, few_shots)\n",
    "print(incorrect_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a65cbe04-692c-458a-91b9-b1d6978fea53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Correct_Loss': 2.8430206775665283,\n",
       " 'Inorrect_Loss': 2.852612257003784,\n",
       " 'pred': 1}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = get_few_shot_pred(text)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "00e78fd7-ea50-4145-835e-6feb21b1c0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "16eef412-78ad-4a45-839a-67e8102918f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bd659c602946558737dde640a0348f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fewshot_preds = []\n",
    "\n",
    "for text in tqdm(test_ds['text']):\n",
    "    out = get_few_shot_pred(text)\n",
    "    fewshot_preds.append(out['pred'])\n",
    "    #print(out,'\\n') ## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06b47317-7809-44fd-9e72-23c730198d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fewshot_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "93d6a237-942a-4d0b-ae1a-1d15e73930ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.12      0.15       250\n",
      "           1       0.74      0.84      0.79       733\n",
      "\n",
      "    accuracy                           0.66       983\n",
      "   macro avg       0.47      0.48      0.47       983\n",
      "weighted avg       0.60      0.66      0.63       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(eval_targets, fewshot_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e85d58c0-6b8e-41f2-b925-720218099e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.66\n",
      "Precision: 0.738\n",
      "   Recall: 0.844\n",
      " F1-score: 0.788\n",
      "      AUC: 0.482\n"
     ]
    }
   ],
   "source": [
    "results['Few-shots'] = quality(eval_targets, fewshot_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d392ade-7468-4cd7-9b88-559513ebe86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ruBERT</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.747</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zero-shot</th>\n",
       "      <td>0.732</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Few-shots</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Accuracy  Precision  Recall  F1-score    AUC\n",
       "ruBERT        0.748      0.747   1.000     0.855  0.504\n",
       "Zero-shot     0.732      0.751   0.959     0.842  0.514\n",
       "Few-shots     0.660      0.738   0.844     0.788  0.482"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "325a831d-e827-466b-aae3-b4b2df54bb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1536)\n",
       "    (wpe): Embedding(2048, 1536)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=4608, nx=1536)\n",
       "          (c_proj): Conv1D(nf=1536, nx=1536)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=6144, nx=1536)\n",
       "          (c_proj): Conv1D(nf=1536, nx=6144)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfd98e-f1de-47a0-9a75-5f33b8d20c43",
   "metadata": {},
   "source": [
    "Загадочно, но с few-shots подходом результаты хуже, по сравнению с zero-shot - возможно, надо дополнительно поиграться с примерами в промпте."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c56b03-d0a0-4ea5-b8f5-e8724185a00f",
   "metadata": {},
   "source": [
    "## RuT5 finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6370e3-97c1-4124-84c7-aea4bec45e18",
   "metadata": {},
   "source": [
    "Обучите и протестируйте модель [RuT5](https://huggingface.co/ai-forever/ruT5-base) на данной задаче - пример finetun’а можете найти [здесь](https://github.com/RussianNLP/RuCoLA/blob/main/baselines/finetune_t5.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31c0a7-4665-4312-93a5-da62e3687bff",
   "metadata": {},
   "source": [
    "RuT5 finetuning\n",
    "```python\n",
    "python baselines/finetune_t5.py -m [MODEL_NAME]\n",
    "```\n",
    "Afterwards, you can get test set predictions in the format required by the leaderboard for all trained models. To do this, run \n",
    "```python\n",
    "python baselines/get_csv_predictions.py -m MODEL1 MODEL2 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "80fbb549-abca-4d5f-90dc-f983cceb2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining metrics\n",
    "ACCURACY = load_metric(\"accuracy\", keep_in_memory=True)\n",
    "PRECISION = load_metric(\"precision\", keep_in_memory=True)\n",
    "RECALL = load_metric(\"recall\", keep_in_memory=True)\n",
    "F1 = load_metric(\"f1\", keep_in_memory=True)\n",
    "ROC_AUC = load_metric(\"roc_auc\", keep_in_memory=True)\n",
    "MCC = load_metric(\"matthews_correlation\", keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b7b132f2-1c78-4543-9d1e-d4b488ab33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining function to compute metrics\n",
    "def compute_metrics(p, tokenizer):\n",
    "    string_preds = tokenizer.batch_decode(p.predictions, skip_special_tokens=True)\n",
    "    int_preds = [1 if prediction == POS_LABEL else 0 for prediction in string_preds]\n",
    "\n",
    "    labels = np.where(p.label_ids != -100, p.label_ids, tokenizer.pad_token_id)\n",
    "    string_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    int_labels = []\n",
    "\n",
    "    for string_label in string_labels:\n",
    "        if string_label == POS_LABEL:\n",
    "            int_labels.append(1)\n",
    "        elif string_label == NEG_LABEL or string_label == \"\":  # second case accounts for test data\n",
    "            int_labels.append(0)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "    acc_result = ACCURACY.compute(predictions=int_preds, references=int_labels)\n",
    "    precision_result = PRECISION.compute(predictions=int_preds, references=int_labels)\n",
    "    recall_result = RECALL.compute(predictions=int_preds, references=int_labels)\n",
    "    f1_result = F1.compute(predictions=int_preds, references=int_labels)\n",
    "    #auc_result = ROC_AUC.compute(predictions=int_preds, references=int_labels)\n",
    "    #mcc_result = MCC.compute(predictions=int_preds, references=int_labels)\n",
    "    \n",
    "    result = {\"accuracy\": acc_result[\"accuracy\"], \n",
    "              \"precision\": precision_result[\"precision\"],\n",
    "              \"recall\": recall_result[\"recall\"],\n",
    "              \"F1-score\": f1_result[\"f1\"],\n",
    "              \"AUC\": 0.5 ##auc_result[\"roc_auc\"]\n",
    "             }\n",
    "    ## Debugging intermediate results on each step \n",
    "    ##results['RuT5-in'] = quality(int_labels, int_preds)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fcd369ce-0609-4ae5-a1b7-443b0057a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t5_name = \"ai-forever/ruT5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "559cfd1c-99e2-4adb-8e58-4f0f2faaa438",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SEEDS = 1\n",
    "N_EPOCHS = 4\n",
    "LR_VALUES = (1e-3,)\n",
    "DECAY_VALUES = (1e-4,)\n",
    "BATCH_SIZES = (128,)\n",
    "\n",
    "POS_LABEL = \"yes\"\n",
    "NEG_LABEL = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7317a1b6-3e6e-4473-96cf-605ef2cf4f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_t5_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c00c8b5a-82d4-4c2d-b0bb-cd85c976e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "TRAIN_FILE = DATA_DIR + \"/\" + \"in_domain_train.csv\"\n",
    "IN_DOMAIN_DEV_FILE = DATA_DIR + \"/\" + \"in_domain_dev.csv\"\n",
    "OUT_OF_DOMAIN_DEV_FILE = DATA_DIR + \"/\" + \"out_of_domain_dev.csv\"\n",
    "TEST_FILE = DATA_DIR + \"/\" + \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f326663e-17cd-4972-ab75-9c613bc1614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            TRAIN_FILE -> ./data/in_domain_train.csv\n",
      "    IN_DOMAIN_DEV_FILE -> ./data/in_domain_dev.csv\n",
      "OUT_OF_DOMAIN_DEV_FILE -> ./data/out_of_domain_dev.csv\n",
      "             TEST_FILE -> ./data/test.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"            TRAIN_FILE -> {TRAIN_FILE}\")\n",
    "print(f\"    IN_DOMAIN_DEV_FILE -> {IN_DOMAIN_DEV_FILE}\")\n",
    "print(f\"OUT_OF_DOMAIN_DEV_FILE -> {OUT_OF_DOMAIN_DEV_FILE}\")\n",
    "print(f\"             TEST_FILE -> {TEST_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4dc47a79-4237-4bf6-8472-1f6a4d48855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_splits(*, as_datasets):\n",
    "    train_df, test_df = map(\n",
    "        pd.read_csv, (TRAIN_FILE, IN_DOMAIN_DEV_FILE)\n",
    "    )\n",
    "\n",
    "    # concatenate datasets to get aggregate metrics\n",
    "    #dev_df = pd.concat((in_domain_dev_df, out_of_domain_dev_df))\n",
    "\n",
    "    if as_datasets:\n",
    "        train, test = map(Dataset.from_pandas, (train_df, test_df))\n",
    "        return DatasetDict(train=train, test=test)\n",
    "    else:\n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3ea5622a-10ba-45cc-86d8-a14169ed2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to prepare datasets here\n",
    "splits = read_splits(as_datasets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24022587-2e25-4a50-82bc-30ce63296baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
       "        num_rows: 7869\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
       "        num_rows: 983\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3a09633d-5dce-4d7c-9ff5-5a8fdfd3025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_examples(examples, tokenizer):\n",
    "    result = tokenizer(examples[\"sentence\"], padding=False)\n",
    "\n",
    "    if \"acceptable\" in examples:\n",
    "        label_sequences = []\n",
    "        for label in examples[\"acceptable\"]:\n",
    "            if label == 1:\n",
    "                target_sequence = POS_LABEL\n",
    "            elif label == 0:\n",
    "                target_sequence = NEG_LABEL\n",
    "            else:\n",
    "                raise ValueError(\"Unknown class label\")\n",
    "            label_sequences.append(target_sequence)\n",
    "\n",
    "    else:\n",
    "        # a hack to avoid the \"You have to specify either decoder_input_ids or decoder_inputs_embeds\" error\n",
    "        # for test data\n",
    "        label_sequences = [\"\" for _ in examples[\"sentence\"]]\n",
    "\n",
    "    result[\"labels\"] = tokenizer(label_sequences, padding=False)[\"input_ids\"]\n",
    "    result[\"length\"] = [len(list(tokenize(sentence))) for sentence in examples[\"sentence\"]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "77180bd6-6e7d-4126-954b-708743b3c076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792a8f3575d74afd8565e1820ce2cbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3996801a3cf0448999f334c76b4d2abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/983 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizing our dataset\n",
    "tokenized_splits = splits.map(\n",
    "        partial(preprocess_examples, tokenizer=tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=[\"sentence\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "10f6c880-3af2-4ac6-bee5-d484f81c5dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'acceptable', 'error_type', 'detailed_source', 'input_ids', 'attention_mask', 'labels', 'length'],\n",
       "        num_rows: 7869\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'acceptable', 'error_type', 'detailed_source', 'input_ids', 'attention_mask', 'labels', 'length'],\n",
       "        num_rows: 983\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "60acdeeb-46c7-4d13-8f03-a1a9ce10d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e6f29d1d-141f-4e2c-88bc-f0e02b279019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LR_VALUES), len(DECAY_VALUES), len(BATCH_SIZES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "93d2de00-2c9a-4606-b606-85347ec980c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed, lr, wd, bs\n",
    "dev_metrics_per_run = np.empty((N_SEEDS, len(LR_VALUES), len(DECAY_VALUES), len(BATCH_SIZES), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "178e2ae9-904b-4673-b356-96f7c02c3d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, 5)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_metrics_per_run.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e8579077-b56a-462f-baec-83cad32a0407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 01:34, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.521325</td>\n",
       "      <td>0.745677</td>\n",
       "      <td>0.745677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854312</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.364138</td>\n",
       "      <td>0.745677</td>\n",
       "      <td>0.745677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854312</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.279618</td>\n",
       "      <td>0.745677</td>\n",
       "      <td>0.745677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854312</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.280039</td>\n",
       "      <td>0.745677</td>\n",
       "      <td>0.745677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854312</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.746\n",
      "Precision: 0.746\n",
      "   Recall: 1.0\n",
      " F1-score: 0.854\n",
      "      AUC: 0.5\n",
      " Accuracy: 0.746\n",
      "Precision: 0.746\n",
      "   Recall: 1.0\n",
      " F1-score: 0.854\n",
      "      AUC: 0.5\n",
      " Accuracy: 0.746\n",
      "Precision: 0.746\n",
      "   Recall: 1.0\n",
      " F1-score: 0.854\n",
      "      AUC: 0.5\n",
      " Accuracy: 0.746\n",
      "Precision: 0.746\n",
      "   Recall: 1.0\n",
      " F1-score: 0.854\n",
      "      AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai-forever/ruT5-large_0.001_0.0001_128_0\n",
      "train {'train_runtime': 94.9664, 'train_samples_per_second': 331.444, 'train_steps_per_second': 2.611, 'total_flos': 3393982267392000.0, 'train_loss': 0.7644554876512096, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.746\n",
      "Precision: 0.746\n",
      "   Recall: 1.0\n",
      " F1-score: 0.854\n",
      "      AUC: 0.5\n",
      "test {'test_loss': 0.5213252902030945, 'test_accuracy': 0.745676500508647, 'test_precision': 0.745676500508647, 'test_recall': 1.0, 'test_F1-score': 0.8543123543123543, 'test_AUC': 0.5, 'test_runtime': 3.366, 'test_samples_per_second': 292.042, 'test_steps_per_second': 2.377}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.746\n",
      "Precision: 0.746\n",
      "   Recall: 1.0\n",
      " F1-score: 0.854\n",
      "      AUC: 0.5\n",
      " Accuracy: 0.746\n",
      "Precision: 0.746\n",
      "   Recall: 1.0\n",
      " F1-score: 0.854\n",
      "      AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "for i, learning_rate in enumerate(LR_VALUES):\n",
    "    for j, weight_decay in enumerate(DECAY_VALUES):\n",
    "        for k, batch_size in enumerate(BATCH_SIZES):\n",
    "            for seed in range(N_SEEDS):\n",
    "                model = T5ForConditionalGeneration.from_pretrained(model_t5_name)\n",
    "\n",
    "                run_base_dir = f\"{model_t5_name}_{learning_rate}_{weight_decay}_{batch_size}\"\n",
    "\n",
    "                training_args = Seq2SeqTrainingArguments(\n",
    "                    output_dir=f\"checkpoints/{run_base_dir}\",\n",
    "                    overwrite_output_dir=True,\n",
    "                    evaluation_strategy=\"epoch\",\n",
    "                    per_device_train_batch_size=batch_size,\n",
    "                    per_device_eval_batch_size=batch_size,\n",
    "                    learning_rate=learning_rate,\n",
    "                    weight_decay=weight_decay,\n",
    "                    num_train_epochs=N_EPOCHS,\n",
    "                    lr_scheduler_type=\"constant\",\n",
    "                    save_strategy=\"epoch\",\n",
    "                    save_total_limit=1,\n",
    "                    seed=seed,\n",
    "                    fp16=True,\n",
    "                    dataloader_num_workers=4,\n",
    "                    group_by_length=True,\n",
    "                    report_to=\"none\",\n",
    "                    load_best_model_at_end=True,\n",
    "                    metric_for_best_model=\"eval_F1-score\", ##\"eval_mcc\",\n",
    "                    optim=\"adafactor\",\n",
    "                    predict_with_generate=True,\n",
    "                )\n",
    "\n",
    "                trainer = Seq2SeqTrainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=tokenized_splits[\"train\"],\n",
    "                    eval_dataset=tokenized_splits[\"test\"],\n",
    "                    compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "                    tokenizer=tokenizer,\n",
    "                    data_collator=data_collator,\n",
    "                )\n",
    "\n",
    "                train_result = trainer.train()\n",
    "                print(f\"{run_base_dir}_{seed}\")\n",
    "                print(\"train\", train_result.metrics)\n",
    "\n",
    "                #os.makedirs(f\"results/{run_base_dir}_{seed}\", exist_ok=True)\n",
    "\n",
    "                dev_predictions = trainer.predict(\n",
    "                    test_dataset=tokenized_splits[\"test\"], metric_key_prefix=\"test\", max_length=10\n",
    "                )\n",
    "                print(\"test\", dev_predictions.metrics)\n",
    "                dev_metrics_per_run[seed, i, j, k] = (\n",
    "                    dev_predictions.metrics[\"test_accuracy\"],\n",
    "                    dev_predictions.metrics[\"test_precision\"],\n",
    "                    dev_predictions.metrics[\"test_recall\"],\n",
    "                    dev_predictions.metrics[\"test_F1-score\"],\n",
    "                    dev_predictions.metrics[\"test_AUC\"],\n",
    "                    #dev_predictions.metrics[\"test_mcc\"],\n",
    "                )\n",
    "\n",
    "                predictions = trainer.predict(test_dataset=tokenized_splits[\"test\"], max_length=10)\n",
    "\n",
    "                string_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
    "\n",
    "                int_preds = [1 if prediction == POS_LABEL else 0 for prediction in string_preds]\n",
    "                int_preds = np.asarray(int_preds)\n",
    "                # Calculating metrics\n",
    "                results['RuT5'] = quality(eval_targets, int_preds)\n",
    "\n",
    "                #np.save(f\"results/{run_base_dir}_{seed}/preds.npy\", int_preds)\n",
    "\n",
    "                #rmtree(f\"checkpoints/{run_base_dir}\")\n",
    "\n",
    "#os.makedirs(\"results_agg\", exist_ok=True)\n",
    "#np.save(f\"results_agg/{model_name}_dev.npy\", dev_metrics_per_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583033f-4c37-44ca-956d-5512d5871a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108af673-0de2-4900-97b7-ba91c074d8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdb912-4812-4078-ba33-cfa979e1d07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d21e929b-9bc7-4c9b-9095-9dbfbf78d6d3",
   "metadata": {},
   "source": [
    "## Итоговое сравнение полученных результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a401f0d-a501-4b32-84fb-816cc79f46b6",
   "metadata": {},
   "source": [
    "Отсортируем полученные результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "05a12c79-146e-4a34-9f65-7da6410e744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ruBERT</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.747</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuT5-in</th>\n",
       "      <td>0.746</td>\n",
       "      <td>0.746</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuT5</th>\n",
       "      <td>0.746</td>\n",
       "      <td>0.746</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zero-shot</th>\n",
       "      <td>0.732</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Few-shots</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Accuracy  Precision  Recall  F1-score    AUC\n",
       "ruBERT        0.748      0.747   1.000     0.855  0.504\n",
       "RuT5-in       0.746      0.746   1.000     0.854  0.500\n",
       "RuT5          0.746      0.746   1.000     0.854  0.500\n",
       "Zero-shot     0.732      0.751   0.959     0.842  0.514\n",
       "Few-shots     0.660      0.738   0.844     0.788  0.482"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC']).T.sort_values(by=['F1-score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e500b12-1178-40fe-8e6a-d8dc318c0431",
   "metadata": {},
   "source": [
    "Осталось\n",
    "- Удалить ROC_AUC из состава метрик;\n",
    "- вернуть 5 эпох в обучение ruBERT;\n",
    "- написать итоговые выводы;\n",
    "- удалить отладочные/промежуточные комментарии;\n",
    "- сохранить jupyter notebook;\n",
    "- загрузить в GitHub финальную версию;\n",
    "- отправить ДЗ на проверку;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e4609-22fa-4c57-83b9-c40a64fc1706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
