{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72a8ab1-6ab4-4d84-b86a-11aac12d9f0e",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "LoRA и доменная адаптация\n",
    "\n",
    "**Цель:**\n",
    "\n",
    "В данном домашнем задании вам предстоит применять подход на основе LoRA для адаптации языковых моделей под различные задачи, дообучать языковые модели с помощью LoRA.\n",
    "\n",
    "### Описание/Пошаговая инструкция выполнения домашнего задания:\n",
    "\n",
    "1. Возьмите датасет для задачи саммаризации. Например, https://huggingface.co/datasets/EdinburghNLP/xsum или похожий по структуре на ваш выбор; для данного задания оставьте 10K-20K случайных примеров.\n",
    "\n",
    "2. Выберите одну из генеративных моделей\n",
    "   - https://huggingface.co/bigscience/bloom-1b1\n",
    "   - https://huggingface.co/FacebookAI/roberta-large\n",
    "    - https://huggingface.co/meta-llama/Llama-3.2-1B\n",
    "    - https://huggingface.co/thepowerfuldeez/Qwen2-1.5B-Summarize\n",
    "\n",
    "(если позволяют ресурсы, можно выбрать и другую модель бОльшего размера)\n",
    "\n",
    "3. Подготовка данных\n",
    "   - при необходимости предобработайте текст - обрежьте входные данные до лимита токена выбранной модели;\n",
    "   - разделите данные на обучающую и валидационную выборки в соотношении 80/20;\n",
    "\n",
    "4. Выберите метрику, по которой будете оценивать результаты обучения - ROUGE, BLEU, BERT score или что-то другое. Опишите мотивацию своего выбора.\n",
    "\n",
    "5. Fine-tuning с помощью LoRA\n",
    ">\n",
    "А) используйте PEFT с методом LoRA или VB-LoRA\n",
    ">\n",
    "Б) сделайте несколько экспериментов обучения с разными гиперпараметрами\n",
    "    - batch size: пара значений на ваш выбор, например 128 и 256\n",
    "    - rank: два-три значения на ваш выбор, например 4, 16, 64\n",
    "\n",
    "6. Оценка качества\n",
    "   - выведите таблицу полученных метрик для каждой пары гиперпараметров\n",
    "   - продемонстрируйте несколько предсказаний лучшей модели\n",
    "   - если лучшие метрики показались неудовлетворительными, опишите, какие шаги можно было сделать по-другому, чтобы улучшить результат.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13df20f-f814-441f-bca4-897c67e8a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling ##DataCollatorWithPadding\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8db0b5-b9b0-4db5-97c0-9d309fd2082b",
   "metadata": {},
   "source": [
    "## 1. Выбор модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12873f5f-4ef3-4bd8-9ad8-f4c890cbfc6d",
   "metadata": {},
   "source": [
    "В качестве модели выберем bigscience/bloomz-560m, модели большего размера в процессе обучения падают по out of memory на 24 Gb VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed52200-7a90-4935-b6e1-154a9ddf14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model = \"bigscience/bloom-1b1\"\n",
    "base_model = \"bigscience/bloomz-560m\"\n",
    "#base_model=\"FacebookAI/roberta-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea4da22c-f1c5-4884-909d-7f4bf485f045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa8ed08-5ae1-4b13-82d9-f43eed9bbc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='bigscience/bloomz-560m', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e88380-ab46-4f70-9c07-bfabfba0a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ac7aa3-fef6-42a4-a898-5c771ec82a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 1024)\n",
       "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86480017-dd98-414b-a947-0f02c4af78e3",
   "metadata": {},
   "source": [
    "## 2. Подготовка даных\n",
    "\n",
    "В качестве набора данных буду использовать [CNN / Daily Mail](https://huggingface.co/datasets/abisee/cnn_dailymail/tree/main/3.0.0) из предварительно загруженных локальных csv файлов. \n",
    "\n",
    "Попытка загрузки [Extreme Summarization (XSum) Dataset](https://huggingface.co/datasets/EdinburghNLP/xsum) или даже просто получение информации о наборе данных, приводит к ошибке:\n",
    "> \"UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 4: invalid continuation byte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f1f977-09b8-4276-88a2-18cb3718c20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': './data/cnn_dailymail/train.csv',\n",
       " 'validation': './data/cnn_dailymail/validation.csv',\n",
       " 'test': './data/cnn_dailymail/test.csv'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/cnn_dailymail/\"\n",
    "data_files={\"train\": path+\"train.csv\", \"validation\": path+\"validation.csv\", \"test\": path+\"test.csv\"}\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc13e43-7013-41b5-9bb7-45826b8c3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_datasets = load_dataset(\"csv\", data_files=data_files, split=[\"train[:800]\",\"validation[:200]\"]) ## to get ~80/20 train/val split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d19823c4-0a5a-4936-9abc-b5966d53248f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['id', 'article', 'highlights'],\n",
       "     num_rows: 800\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'article', 'highlights'],\n",
       "     num_rows: 200\n",
       " })]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9998bfae-61a3-4268-b50a-ed56f2009c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## renaming and removing columns\n",
    "for i in range(0, len(temp_datasets)):\n",
    "    temp_datasets[i] = temp_datasets[i].rename_column(\"article\",\"text\")\n",
    "    temp_datasets[i] = temp_datasets[i].rename_column(\"highlights\", \"target\")\n",
    "    temp_datasets[i] = temp_datasets[i].remove_columns(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "473fd08b-6f68-47ac-aa73-c4cc8f6706cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['text', 'target'],\n",
       "     num_rows: 800\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'target'],\n",
       "     num_rows: 200\n",
       " })]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22a0a0c6-9898-4059-bbce-683d0ab37e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_datasets[0] = temp_datasets[0].map(lambda row: tokenizer(row['text']), batched=True)\n",
    "temp_datasets[1] = temp_datasets[1].map(lambda row: tokenizer(row['text']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb02b98f-7d58-4382-9431-de47dcc6da75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'target', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'target', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a single DatasetDict for easy access\n",
    "data = DatasetDict({\n",
    "    \"train\": temp_datasets[0],\n",
    "    \"validation\": temp_datasets[1],\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c482e666-268b-4db7-8dc9-8dd7edb9ca1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\",\n",
       " 'Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\\nHe contracted the infection through contaminated food in Italy .\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]['text'], data[\"train\"][0]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92679709-e30d-4598-9958-eba7a0ace333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing memory from temporary datasets\n",
    "del temp_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc22287-8d2c-4121-a69b-a8b9efaaefda",
   "metadata": {},
   "source": [
    "## 4. Выбор метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4bde2-828f-41a9-9318-0d81c6ff7cb9",
   "metadata": {},
   "source": [
    "- BLEU (Bilingual Evaluation Understudy) больше подходит, когда важно оценить точность (precision) перевода, как например, в задаче машинного перевода, для которой данная метрика и была создана изначально;\n",
    "  \n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation) использует recall, precision и F-меру для рценки соответствия последовательностей слов (n-grams) между сгенерированным текстом и референсным написанным человеком текстом. Хорошо подходит для задач суммаризации текста, где цель - извлечь как можно больше ключевой информации из исходного документа.\n",
    "\n",
    "- BERTscore использует контекстные эмбеддинги современных LLM, таких как BERT. Применяется для оценки семантической точности между сгенерированным и исходным текстом и подходит для решения широкого круга задач.\n",
    "\n",
    "Будем использовать метрику ROUGE, как наиболее подходящую для задачи суммаризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "833af447-1871-4977-b9cb-da7031d0cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d76cc-889e-4019-81ac-045adaa639b4",
   "metadata": {},
   "source": [
    "## 5.Fine-tuning с помощью LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf5cdd7-c224-4c5e-83ac-d93d5fb41958",
   "metadata": {},
   "source": [
    "Даже маленькая модель на 0.5B параметров на обучении поместилась в 24Gb VRAM только с batch_size=1. В связи с отсутствием возможности варьировать batch_size, протестируем модель на трех значениях ранга: 4, 16 и 64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d2e9c-93a8-4f23-b9ae-62e1dea85d1c",
   "metadata": {},
   "source": [
    "## 5.1 rank = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ba82902-99d7-406d-9e77-289c02f7d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e974bfae-3eca-4adf-b5f6-91db17b1528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=RANK, # As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    target_modules=[\"query_key_value\"], # You can obtain a list of target modules in the URL above.\n",
    "    lora_dropout=0.05, # Helps to avoid Overfitting.\n",
    "    bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fc9bb60-a009-4624-b8c4-a61eace676df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 466,944 || all params: 559,607,808 || trainable%: 0.0834\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "527bd94a-3795-41df-8ddf-8cba074e13ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./peft_lab_outputs'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = './'\n",
    "output_directory = os.path.join(working_dir, 'peft_lab_outputs')\n",
    "output_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9aa9cb3-62fa-4874-a549-e997deeb7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the TrainingArgs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    per_device_train_batch_size=1,  # Batch size per device for training\n",
    "    per_device_eval_batch_size=1,   # Batch size per device for evaluating   \n",
    "    #auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=4,\n",
    "    #use_cpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9619b6e8-a974-4ff1-83ae-a6fe0441c233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "908082a1-734a-4347-920c-27450a051c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 06:15, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>12.853800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>9.538100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>8.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>8.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>8.063200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3200, training_loss=9.188214950561523, metrics={'train_runtime': 375.7885, 'train_samples_per_second': 8.515, 'train_steps_per_second': 8.515, 'total_flos': 5102755699458048.0, 'train_loss': 9.188214950561523, 'epoch': 4.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63c9ac41-b1b5-4132-a390-5551c02450a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "peft_model_path = os.path.join(output_directory, f'lora_model')\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ee009ca-3b3f-4f8a-b17c-38b86f225b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BloomForCausalLM(\n",
       "      (transformer): BloomModel(\n",
       "        (word_embeddings): Embedding(250880, 1024)\n",
       "        (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Model\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_path, is_trainable=False)\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd12bb89-9aab-4dfe-86ba-f2638a778f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b532044-a2a8-4586-a0cf-4b1398ce2567",
   "metadata": {},
   "source": [
    "### Расчет метрики ROUGE для rank=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ca3d3ee-c694-41d3-afb4-763b92efb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "predictions = []\n",
    "references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "764848bf-c27c-4fd4-8a8b-b52acce1f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the outputs from the model received, and inputs.\n",
    "def get_outputs(model, inputs, max_new_tokens=100):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5, # Avoid repetition.\n",
    "        early_stopping=True, # The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8395cc83-b1e1-4b55-be1c-a5cf4c4c55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality(model, max_new_tokens=50):\n",
    "    #predictions = []\n",
    "    #references = []\n",
    "    \n",
    "    for item in tqdm(data[\"validation\"]):\n",
    "        input_ids = tokenizer(item['text'], return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "        output_ids = get_outputs(model, input_ids, max_new_tokens=50)\n",
    "        prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        #Saving predictions and true labels \n",
    "        predictions.append(prediction)\n",
    "        references.append(item['target'])\n",
    "        \n",
    "    # Calculating ROUGE metric\n",
    "    rouge_dict = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    print('Evaluation:')\n",
    "    for k, v in rouge_dict.items():\n",
    "        print('\\t{} = {:.5f}'.format(k, v))\n",
    "\n",
    "    return [rouge_dict[k] for k in rouge_dict]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1602fbfd-d5d2-4e9a-8454-290a57e9797b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9a5036d9e54d56add5d59da9cda8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "\trouge1 = 0.19643\n",
      "\trouge2 = 0.10424\n",
      "\trougeL = 0.13192\n",
      "\trougeLsum = 0.16649\n"
     ]
    }
   ],
   "source": [
    "results['rank='+f\"{RANK}\"] = quality(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d4042af-4647-4691-bb19-9dba6dc2bcd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rank=4</th>\n",
       "      <td>0.196428</td>\n",
       "      <td>0.104236</td>\n",
       "      <td>0.131919</td>\n",
       "      <td>0.166488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rouge1    rouge2    rougeL  rougeLsum\n",
       "rank=4  0.196428  0.104236  0.131919   0.166488"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index=['rouge1', 'rouge2', 'rougeL', 'rougeLsum']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1e93ec4-4499-4e71-9dcb-a5b6b4421ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sally Forrest, an actress-dancer who graced the silver screen throughout the \\'40s and \\'50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film \\'Hard, Fast and Beautiful\\' (left) and the 1956 Fritz Lang movie \\'While the City Sleeps\\' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest\\'s other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB\\xa0page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films . said with it he his be have been from has this Iing\" but them were their not out up which are one about they after over - In But into where all last more ,The We or will years there some when we’s ‘ Mr They',\n",
       " 200)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of prediction\n",
    "predictions[0], len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed836f8f-fb16-4b18-b093-7c17dcb676a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films died on March 15 .\\nForrest, whose birth name was Katherine Feeney, had long battled cancer .\\nA San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .\",\n",
       " 200)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference example\n",
    "references[0], len(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad5a66-fc68-43cf-8e19-4f195b7c6cac",
   "metadata": {},
   "source": [
    "## 5.2 rank=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "512350d1-56a1-4047-a551-adbecae06df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8be78c3b-2867-4644-a412-3db3bdb76f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=RANK, # As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    target_modules=[\"query_key_value\"], # You can obtain a list of target modules in the URL above.\n",
    "    lora_dropout=0.05, # Helps to avoid Overfitting.\n",
    "    bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81a26c8c-3325-494a-bf65-d60b876370dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,646,592 || all params: 560,787,456 || trainable%: 0.2936\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "del peft_model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "745741fe-b113-42a5-bbfb-5f72f45aa5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "522d5510-30e3-4549-adb7-b1c8fe86a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    per_device_train_batch_size=1,  # Batch size per device for training\n",
    "    per_device_eval_batch_size=1,   # Batch size per device for evaluating   \n",
    "    #auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=4,\n",
    "    #use_cpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5abd05a7-a82d-44c0-b306-fe6c5c213210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 06:20, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>15.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>9.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>8.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>7.828300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.759400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3200, training_loss=9.492246704101563, metrics={'train_runtime': 380.6493, 'train_samples_per_second': 8.407, 'train_steps_per_second': 8.407, 'total_flos': 5122641139040256.0, 'train_loss': 9.492246704101563, 'epoch': 4.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f3257ba-6a0d-4d98-9452-20fd0e718940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "peft_model_path = os.path.join(output_directory, f'lora_model{RANK}')\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6d392f3-9f1a-4dc8-9066-2d72ccfe5b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BloomForCausalLM(\n",
       "      (transformer): BloomModel(\n",
       "        (word_embeddings): Embedding(250880, 1024)\n",
       "        (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Model\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_path, is_trainable=False)\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0fa88fd-f350-4eb9-a0d7-34cac3ef05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64165c73-9fe2-481e-9399-fa387e679c83",
   "metadata": {},
   "source": [
    "### Расчет метрики ROUGE для rank=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e4c808a-4517-4bf5-8463-2bfe911d2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fec6259a-292d-4ce7-a8da-0c1146e20333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930eb14eade04b2a86be676a7326a1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "\trouge1 = 0.19650\n",
      "\trouge2 = 0.10399\n",
      "\trougeL = 0.13186\n",
      "\trougeLsum = 0.16624\n"
     ]
    }
   ],
   "source": [
    "results['rank='+f\"{RANK}\"] = quality(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ef8630f-5c49-48b5-b396-7d6d19ecaff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sally Forrest, an actress-dancer who graced the silver screen throughout the \\'40s and \\'50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film \\'Hard, Fast and Beautiful\\' (left) and the 1956 Fritz Lang movie \\'While the City Sleeps\\' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest\\'s other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB\\xa0page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films . he with it his from said I has were haveing not be one been out this after are\" they their all but In \" she -’s now there will which them time lastThe could But so more or before two some over about when years up',\n",
       " 200)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of prediction\n",
    "predictions[0], len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17b8bad9-b88f-4531-91a8-0925cd4557ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films died on March 15 .\\nForrest, whose birth name was Katherine Feeney, had long battled cancer .\\nA San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .\",\n",
       " 200)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference example\n",
    "references[0], len(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb602c-4da9-4179-b681-938bb94fdd9d",
   "metadata": {},
   "source": [
    "### 5.3 rank = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "788b52f7-40d2-42a2-9326-28529d612f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c1738c7-cd97-46dc-86f2-80f4cc80bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=RANK, # As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    target_modules=[\"query_key_value\"], # You can obtain a list of target modules in the URL above.\n",
    "    lora_dropout=0.05, # Helps to avoid Overfitting.\n",
    "    bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53946c8b-c750-4370-8f42-ce44a2b372b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,365,184 || all params: 565,506,048 || trainable%: 1.1256\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "del peft_model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0adc9c0-b1e0-4f95-809e-dd4396653053",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b83b0aa-e99d-461a-a319-9d1e0beb6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    per_device_train_batch_size=1,  # Batch size per device for training\n",
    "    per_device_eval_batch_size=1,   # Batch size per device for evaluating   \n",
    "    #auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=4,\n",
    "    #use_cpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac265b37-5a4d-4213-a5bb-7dd48184f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 06:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>11.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>8.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>7.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.789200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3200, training_loss=8.557541198730469, metrics={'train_runtime': 383.1796, 'train_samples_per_second': 8.351, 'train_steps_per_second': 8.351, 'total_flos': 5202182897369088.0, 'train_loss': 8.557541198730469, 'epoch': 4.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "628caea5-b5c3-4096-b682-e75a70cc55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "peft_model_path = os.path.join(output_directory, f'lora_model{RANK}')\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f733ca6-8e31-4da9-a21f-c55b2843230c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BloomForCausalLM(\n",
       "      (transformer): BloomModel(\n",
       "        (word_embeddings): Embedding(250880, 1024)\n",
       "        (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Model\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_path, is_trainable=False)\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59679c88-d52b-4bd3-a9cc-7c2519a5a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d8114-3154-4265-a9b0-509aa0670b96",
   "metadata": {},
   "source": [
    "### Расчет метрики ROUGE для rank=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30ad0087-43cd-4c2f-bd0b-5112b9b3f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2399337-391d-4af3-93e2-8742bb7b1873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3690125c2bbc44ddb6f02a5fa7296048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "\trouge1 = 0.19630\n",
      "\trouge2 = 0.10381\n",
      "\trougeL = 0.13150\n",
      "\trougeLsum = 0.16583\n"
     ]
    }
   ],
   "source": [
    "results['rank='+f\"{RANK}\"] = quality(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff813465-30f0-4991-8d55-59e938015b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB\\xa0page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films . with iting his he have said from but has I were been this after not their they one be are will she more about out when them - ‘ can up two which another’s overers or so there some’ under last , In did being day\",\n",
       " 200)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of prediction\n",
    "predictions[0], len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd14eb11-709f-4439-a3d9-c47ea6fb619e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films died on March 15 .\\nForrest, whose birth name was Katherine Feeney, had long battled cancer .\\nA San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .\",\n",
       " 200)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference example\n",
    "references[0], len(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70269950-ad4d-4b8c-b53c-6a653151e526",
   "metadata": {},
   "source": [
    "## 6. Сравнение результатов и выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa504d17-02cb-4f72-b0df-5aab9d38a4e7",
   "metadata": {},
   "source": [
    "Итоговая таблица результатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a710e5a-d67c-4d02-a59c-456324a57dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rank=4</th>\n",
       "      <td>0.196428</td>\n",
       "      <td>0.104236</td>\n",
       "      <td>0.131919</td>\n",
       "      <td>0.166488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank=16</th>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.103992</td>\n",
       "      <td>0.131863</td>\n",
       "      <td>0.166245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank=64</th>\n",
       "      <td>0.196301</td>\n",
       "      <td>0.103811</td>\n",
       "      <td>0.131501</td>\n",
       "      <td>0.165831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1    rouge2    rougeL  rougeLsum\n",
       "rank=4   0.196428  0.104236  0.131919   0.166488\n",
       "rank=16  0.196500  0.103992  0.131863   0.166245\n",
       "rank=64  0.196301  0.103811  0.131501   0.165831"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index=['rouge1', 'rouge2', 'rougeL', 'rougeLsum']).T.sort_values(by=['rougeLsum'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332bbb0-78d5-44bb-8f6a-b2a20ee7fcd1",
   "metadata": {},
   "source": [
    "Результаты достаточно близкие и невысокие - для улучшения результатов, на мой взгляд, будет полезно:\n",
    "- использовать больший объем данных для обучения модели;\n",
    "- обучать в течении большего количества эпох;\n",
    "- подобрать больший размер батча для повышения стабильности процесса обучения;\n",
    "\n",
    "Реализация этих мер потребует уже более серьезных вычислительных ресурсов и времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3e1cc-d98f-450c-8285-f47d1b488b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
